# -*- coding: utf-8 -*-
"""RCCMPE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17iHHmxjk8CEalEBnea4PsIF3W_WBPAtt
"""

import torch, tensorflow as tf
from copy import deepcopy
import os
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from dataclasses import dataclass
import random
import numpy as np
import pandas as pd

from recommenders.utils.constants import (
    DEFAULT_USER_COL,
    DEFAULT_ITEM_COL,
    DEFAULT_PREDICTION_COL,
)
from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF
from recommenders.models.deeprec.deeprec_utils import prepare_hparams
from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN

print("PyTorch CUDA:", torch.cuda.is_available())
print("TF GPUs:", tf.config.list_physical_devices("GPU"))


# Patch for old LightGCN code expecting np.mat (removed in NumPy 2.0)
if not hasattr(np, "mat"):
    def mat(x, *args, **kwargs):
        # Behave close enough for LightGCN's use: just make it a 2D array
        return np.asarray(x, *args, **kwargs)
    np.mat = mat

print("Patched np.mat for compatibility with LightGCN.")

"""## Ishan's code"""




def load_split(path):
    rows = []
    with open(path, "r") as f:
        for line in f:
            line_arr = line.strip().split()
            u = int(line_arr[0])
            for item_str in line_arr[1:]:
                rows.append((u, int(item_str), 1.0))

    df = pd.DataFrame(rows, columns=[DEFAULT_USER_COL,
                                     DEFAULT_ITEM_COL,
                                     DEFAULT_RATING_COL])
    return df

train_df = load_split("data/train.txt")
val_df   = load_split("data/val.txt")
test_df  = load_split("data/test.txt")

train_df = pd.concat([train_df, val_df], ignore_index=True)

data_val = ImplicitCF(
    train=train_df,
    test=test_df,
    seed=1337,
)

best_ndcg = None
best_cfg = None

testing = False
if testing:
    # for emb in [64, 128, 256]:
    for emb in [256]:
        for layers in [2]:
            if emb == 128 and layers != 4:
                continue
            model_dir = f"models/lightgcn_{emb}_{layers}"
            # os.mkdir(model_dir)
            print(f"Running {emb} {layers}")
            hparams = prepare_hparams(
                yaml_file=None,
                MODEL_DIR=model_dir,
                model_type="lightgcn",
                embed_size=emb,
                n_layers=layers,
                decay=1e-4,
                learning_rate=0.001,
                batch_size=2048,
                epochs=100,
                eval_epoch=25,
                top_k=20,
                metrics=["ndcg", "recall"],
                save_model=False,
                # save_model=True,
                save_epoch=100,
            )
            model = LightGCN(hparams, data_val, seed=1337)
            model.fit()
            ndcg_val, recall_val = model.run_eval()
            if best_ndcg is None or ndcg_val > best_ndcg:
                best_ndcg = ndcg_val
                best_cfg = (emb, layers)

    print("Best NDCG:", best_ndcg, "with emb, layers:", best_cfg)

# 64, 2
# ndcg = 0.08441, recall = 0.16314
# 64, 3
# ndcg = 0.08181, recall = 0.15731
# 64, 4
# ndcg = 0.07828, recall = 0.15102
# 128, 2
# ndcg = 0.09196, recall = 0.17580
# 128, 3
# ndcg = 0.08783, recall = 0.16896
# 128, 4
# ndcg = 0.08439, recall = 0.16350
# 256, 2
# ndcg = 0.09858, recall = 0.18768   BEST
# 256, 3
# ndcg = 0.09462, recall = 0.18174
# 256, 4
# ndcg = 0.08968, recall = 0.17345


# Submission file

def create_sub_file(name = "lightgcn_grid_256_2.txt"):

    train_full = pd.concat([train_df, test_df], ignore_index=True)

    data_test = ImplicitCF(
        train=train_full,
        test=None,
        adj_dir=None,
        seed=1337,
    )

    hparams_final = prepare_hparams(
        yaml_file=None,
        MODEL_DIR='',
        model_type="lightgcn",
        eval_epoch=1000,
        embed_size=256,
        n_layers=2,
        decay=1e-4,
        learning_rate=0.001,
        batch_size=2048,
        epochs=100,
        top_k=20,
        metrics=["ndcg", "recall"],
        save_model=False,
        # save_model=True,
        save_epoch=100,
    )

    model_final = LightGCN(hparams_final, data_test, seed=1337)
    model_final.fit()

    user_ids = train_full[DEFAULT_USER_COL].unique()
    user_ids.sort()

    def join_items(x):
        return " ".join(str(i) for i in x.tolist())

    chunk_size = 5000

    with open(f"subs/{name}", "w") as f:
        for start in range(0, len(user_ids), chunk_size):
            end = start + chunk_size
            batch_ids = user_ids[start:end]

            batch_users = pd.DataFrame({DEFAULT_USER_COL: batch_ids})

            batch_recs = model_final.recommend_k_items(
                test=batch_users,
                top_k=20,
                sort_top_k=True,
                remove_seen=True,
                use_id=False,
            )

            batch_submission = (
                batch_recs.sort_values([DEFAULT_USER_COL, DEFAULT_PREDICTION_COL],
                                    ascending=[True, False])
                        .groupby(DEFAULT_USER_COL)[DEFAULT_ITEM_COL]
                        .apply(join_items)
                        .reset_index()
            )

            for _, row in batch_submission.iterrows():
                f.write(f"{int(row[DEFAULT_USER_COL])} {row[DEFAULT_ITEM_COL]}\n")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Core Models

class UserItemEncoder(nn.Module):
    def __init__(self, n_users: int, n_items: int, emb_dim: int):
        super().__init__()
        self.user_emb = nn.Embedding(n_users, emb_dim)
        self.item_emb = nn.Embedding(n_items, emb_dim)

        nn.init.xavier_uniform_(self.user_emb.weight)
        nn.init.xavier_uniform_(self.item_emb.weight)

    def get_user(self, user_ids):
        return self.user_emb(user_ids)

    def get_item(self, item_ids):
        return self.item_emb(item_ids)


class Task1UserFromItems(nn.Module):
    def __init__(self, encoder: UserItemEncoder):
        super().__init__()
        self.encoder = encoder

    def forward(self, item_ids):
        """
        item_ids: LongTensor of shape (B, L) – batch of item sequences
        returns: logits over users of shape (B, n_users)
        """
        # (B, L, D)
        item_embs = self.encoder.get_item(item_ids)
        # pool -> (B, D)
        z = item_embs.mean(dim=1)
        # compare to all user embeddings: (B, D) @ (D, n_users) = (B, n_users)
        user_matrix = self.encoder.user_emb.weight  # (n_users, D)
        logits = z @ user_matrix.t()
        return logits

    def loss(self, item_ids, true_user_ids):
        logits = self.forward(item_ids)
        return F.cross_entropy(logits, true_user_ids)


class Task2RankItems(nn.Module):
    def __init__(self, encoder: UserItemEncoder):
        super().__init__()
        self.encoder = encoder

    def scores(self, user_ids, item_ids):
        u = self.encoder.get_user(user_ids)       # (B, D)
        v = self.encoder.get_item(item_ids)       # (B, D)
        return (u * v).sum(dim=-1)                # (B,)

    def bpr_loss(self, user_ids, pos_item_ids, neg_item_ids):
        pos_score = self.scores(user_ids, pos_item_ids)
        neg_score = self.scores(user_ids, neg_item_ids)
        # BPR loss
        return -torch.log(torch.sigmoid(pos_score - neg_score) + 1e-8).mean()


class MultiTaskPretrainer(nn.Module):
    def __init__(self, encoder, alpha=0.5):
        super().__init__()
        self.encoder = encoder
        self.task1 = Task1UserFromItems(encoder)
        self.task2 = Task2RankItems(encoder)
        self.alpha = alpha

    def step_loss(
        self,
        item_seq_batch,   # (B1, L)
        user_ids_task1,   # (B1,)
        user_ids_task2,   # (B2,)
        pos_items_task2,  # (B2,)
        neg_items_task2,  # (B2,)
    ):
        loss1 = self.task1.loss(item_seq_batch, user_ids_task1)
        loss2 = self.task2.bpr_loss(user_ids_task2, pos_items_task2, neg_items_task2)
        return self.alpha * loss1 + (1 - self.alpha) * loss2


# Genomes (hyperparams for evolution)
@dataclass
class Genome:
    emb_dim: int
    alpha: float
    lr: float
    bpr_weight_decay: float  # example regularization parameter


def random_genome():
    return Genome(
        emb_dim=random.choice([64, 128, 256]),
        alpha=random.uniform(0.05, 0.3),  # <-- smaller CE weight
        lr=10 ** random.uniform(-4, -2),
        bpr_weight_decay=10 ** random.uniform(-6, -3),
    )



def mutate_genome(parent: Genome) -> Genome:
    def mutate_val(v, is_int=False, minv=None, maxv=None):
        factor = 1.0 + random.choice([-0.2, 0.2])
        new_v = v * factor
        if is_int:
            new_v = int(max(1, round(new_v)))
        if minv is not None:
            new_v = max(minv, new_v)
        if maxv is not None:
            new_v = min(maxv, new_v)
        return new_v

    return Genome(
        emb_dim=int(mutate_val(parent.emb_dim, is_int=True, minv=16, maxv=512)),
        alpha=mutate_val(parent.alpha, minv=0.1, maxv=0.9),
        lr=mutate_val(parent.lr, minv=1e-5, maxv=1e-1),
        bpr_weight_decay=mutate_val(parent.bpr_weight_decay, minv=1e-7, maxv=1e-2),
    )


def build_model_from_genome(genome: Genome, n_users, n_items):
    encoder = UserItemEncoder(n_users, n_items, genome.emb_dim).to(device)
    multitask = MultiTaskPretrainer(encoder, alpha=genome.alpha).to(device)
    optimizer = torch.optim.Adam(
        multitask.parameters(),
        lr=genome.lr,
        weight_decay=genome.bpr_weight_decay
    )
    return multitask, optimizer

class Task1Dataset(Dataset):
    """
    For each sample: (sequence_of_items, user_id)
    """
    def __init__(self, user2items, seq_len=5, n_samples=5000):
        self.samples = []
        users = list(user2items.keys())
        for _ in range(n_samples):
            u = random.choice(users)
            items = user2items[u]
            if len(items) >= seq_len:
                seq = random.sample(items, seq_len)
            else:
                # pad by repeating items
                seq = items + random.choices(items, k=seq_len - len(items))
            self.samples.append(
                (torch.tensor(seq, dtype=torch.long),
                 torch.tensor(u, dtype=torch.long))
            )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]


class Task2Dataset(Dataset):
    """
    For each sample: (user_id, pos_item_id, neg_item_id)
    """
    def __init__(self, user2items, n_items, n_samples=5000):
        self.samples = []
        users = list(user2items.keys())
        all_items = set(range(n_items))
        for _ in range(n_samples):
            u = random.choice(users)
            pos_items = user2items[u]
            pos_i = random.choice(pos_items)
            neg_candidates = list(all_items - set(pos_items))
            if len(neg_candidates) == 0:
                neg_i = pos_i
            else:
                neg_i = random.choice(neg_candidates)
            self.samples.append(
                (torch.tensor(u, dtype=torch.long),
                 torch.tensor(pos_i, dtype=torch.long),
                 torch.tensor(neg_i, dtype=torch.long))
            )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]



# Training & Evaluation

def train_for_N_episodes(model, optimizer,
                         train_loader_task1, train_loader_task2, N):
    model.train()
    for epoch in range(N):
        total_loss = 0.0
        n_batches = 0

        for (batch1, batch2) in zip(train_loader_task1, train_loader_task2):
            item_seq, user_ids_t1 = batch1
            u_t2, pos_i, neg_i = batch2

            item_seq = item_seq.to(device)
            user_ids_t1 = user_ids_t1.to(device)
            u_t2 = u_t2.to(device)
            pos_i = pos_i.to(device)
            neg_i = neg_i.to(device)

            optimizer.zero_grad()
            loss = model.step_loss(item_seq, user_ids_t1, u_t2, pos_i, neg_i)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            n_batches += 1

        avg_loss = total_loss / max(1, n_batches)
        print(f"    Episode {epoch+1}/{N} - Train Loss: {avg_loss:.4f}")


@torch.no_grad()
def evaluate_model(model, val_loader_task1, val_loader_task2):
    """
    Simple proxy:
      - Task 1: accuracy in predicting user from items
      - Task 2: average BPR loss (negative is good)
    Return a scalar where higher is better.
    """
    model.eval()

    # --- Task 1 accuracy ---
    correct = 0
    total = 0
    for item_seq, user_ids in val_loader_task1:
        item_seq = item_seq.to(device)
        user_ids = user_ids.to(device)
        logits = model.task1(item_seq)
        preds = logits.argmax(dim=-1)
        correct += (preds == user_ids).sum().item()
        total += user_ids.numel()
    acc1 = correct / max(1, total)

    # --- Task 2 BPR loss ---
    total_loss = 0.0
    total_batches = 0
    for u, pos_i, neg_i in val_loader_task2:
        u = u.to(device)
        pos_i = pos_i.to(device)
        neg_i = neg_i.to(device)
        loss = model.task2.bpr_loss(u, pos_i, neg_i)
        total_loss += loss.item()
        total_batches += 1
    avg_bpr = total_loss / max(1, total_batches)

    # We want higher = better, so subtract avg_bpr
    score = acc1 - avg_bpr
    print(f"    Eval - Task1 Acc: {acc1:.4f}, Task2 BPR Loss: {avg_bpr:.4f}, Score: {score:.4f}")
    return score


def evolution_search(
    n_users,
    n_items,
    train_loader_task1,
    train_loader_task2,
    val_loader_task1,
    val_loader_task2,
    N_episodes,
    generations=3,
    population_size=3,
):
    # initialize population
    population = [random_genome() for _ in range(population_size)]
    best_overall = None  # (score, genome, state_dict)

    for g in range(generations):
        print(f"\n=== Generation {g+1}/{generations} ===")
        scored_population = []

        for i, genome in enumerate(population):
            print(f"  Candidate {i+1}/{len(population)}: {genome}")
            model, optimizer = build_model_from_genome(genome, n_users, n_items)
            train_for_N_episodes(model, optimizer,
                                 train_loader_task1, train_loader_task2, N_episodes)
            score = evaluate_model(model, val_loader_task1, val_loader_task2)
            scored_population.append((score, genome, copy.deepcopy(model.state_dict())))

        # sort descending by score
        scored_population.sort(key=lambda x: x[0], reverse=True)
        elite = scored_population[0]
        print(f"  -> Elite score this gen: {elite[0]:.4f} | Genome: {elite[1]}")

        if (best_overall is None) or (elite[0] > best_overall[0]):
            best_overall = elite

        # next generation population: keep elite genome, plus mutated
        elite_genome = elite[1]
        population = [elite_genome]
        while len(population) < population_size:
            population.append(mutate_genome(elite_genome))

    print("\n=== Evolution finished ===")
    print("Best overall score:", best_overall[0])
    print("Best genome:", best_overall[1])
    return best_overall  # (best_score, best_genome, best_state_dict)

@torch.no_grad()
def evaluate_model(model, val_loader_task1, val_loader_task2, k=50):
    model.eval()

    # --- Task 1: top-k accuracy ---
    correct = 0
    total = 0
    for item_seq, user_ids in val_loader_task1:
        item_seq = item_seq.to(device)
        user_ids = user_ids.to(device)
        logits = model.task1(item_seq)              # (B, n_users)
        topk = logits.topk(k, dim=-1).indices       # (B, k)
        in_topk = (topk == user_ids.unsqueeze(1)).any(dim=1)
        correct += in_topk.sum().item()
        total += user_ids.numel()
    topk_acc = correct / max(1, total)

    # --- Task 2 BPR ---
    total_loss = 0.0
    total_batches = 0
    for u, pos_i, neg_i in val_loader_task2:
        u = u.to(device)
        pos_i = pos_i.to(device)
        neg_i = neg_i.to(device)
        loss = model.task2.bpr_loss(u, pos_i, neg_i)
        total_loss += loss.item()
        total_batches += 1
    avg_bpr = total_loss / max(1, total_batches)

    # Combine:
    score = topk_acc - avg_bpr
    print(f"    Eval - Task1 Top-{k} Acc: {topk_acc:.4f}, Task2 BPR: {avg_bpr:.4f}, Score: {score:.4f}")
    return score


user2items_full = (
    train_df
    .groupby(DEFAULT_USER_COL)[DEFAULT_ITEM_COL]
    .apply(lambda x: x.astype(int).tolist())
    .to_dict()
)

# 2) Infer number of users/items
n_users = max(user2items_full.keys()) + 1
n_items = int(train_df[DEFAULT_ITEM_COL].max()) + 1

print(f"n_users = {n_users}, n_items = {n_items}")
print(f"Average items per user: {sum(len(v) for v in user2items_full.values())/len(user2items_full):.2f}")

# 3) Split users into train/val sets for PRETRAINING
user_list = list(user2items_full.keys())
random.shuffle(user_list)

# we can adjust this split ratio; here ~90% users for "train", 10% for "val"
split = int(0.9 * len(user_list))
train_users = user_list[:split]
val_users   = user_list[split:]

user2items_train = {u: user2items_full[u] for u in train_users}
user2items_val   = {u: user2items_full[u] for u in val_users}

print(f"Pretrain users: {len(train_users)}, Val users: {len(val_users)}")

# 4) Create Task1/Task2 datasets from REAL data
# We can tune seq_len and n_samples as we like
seq_len = 5
train_task1_ds = Task1Dataset(user2items_train, seq_len=seq_len, n_samples=4000)
train_task2_ds = Task2Dataset(user2items_train, n_items=n_items, n_samples=4000)
val_task1_ds   = Task1Dataset(user2items_val,   seq_len=seq_len, n_samples=2000)
val_task2_ds   = Task2Dataset(user2items_val,   n_items=n_items, n_samples=2000)

train_loader_task1 = DataLoader(train_task1_ds, batch_size=128, shuffle=True)
train_loader_task2 = DataLoader(train_task2_ds, batch_size=128, shuffle=True)
val_loader_task1   = DataLoader(val_task1_ds,   batch_size=256, shuffle=False)
val_loader_task2   = DataLoader(val_task2_ds,   batch_size=256, shuffle=False)

print("Data loaders ready. Starting evolutionary pretraining on REAL data ...")

# 5) Run the evolutionary search on our real user2items
best_score, best_genome, best_state = evolution_search(
    n_users=n_users,
    n_items=n_items,
    train_loader_task1=train_loader_task1,
    train_loader_task2=train_loader_task2,
    val_loader_task1=val_loader_task1,
    val_loader_task2=val_loader_task2,
    N_episodes=40,
    generations=6,
    population_size=10,
)

print("\n=== DONE (REAL DATA) ===")
print("  best_genome:", best_genome)
print("  best_score:", best_score)

# best_state is the pre-trained weights we can later load into a new encoder:
# encoder = UserItemEncoder(n_users, n_items, best_genome.emb_dim)
# multitask = MultiTaskPretrainer(encoder, alpha=best_genome.alpha)
# multitask.load_state_dict(best_state)

"""## Converging embeddings to tensorflow from Torch"""

encoder = UserItemEncoder(n_users, n_items, best_genome.emb_dim).to(device)
multitask = MultiTaskPretrainer(encoder, alpha=best_genome.alpha).to(device)
multitask.load_state_dict(best_state)

print(f" Reconstructed pretrained encoder with emb_dim = {best_genome.emb_dim}")
print("   ‖user_emb‖ =", encoder.user_emb.weight.norm().item())
print("   ‖item_emb‖ =", encoder.item_emb.weight.norm().item())

user_emb_np = encoder.user_emb.weight.detach().cpu().numpy()
item_emb_np = encoder.item_emb.weight.detach().cpu().numpy()

np.save("pretrained_user_emb.npy", user_emb_np)
np.save("pretrained_item_emb.npy", item_emb_np)

print("Saved pretrained embeddings to pretrained_user_emb.npy and pretrained_item_emb.npy")



# --- Patch for NumPy 2.0 (np.mat removed) ---
def _convert_sp_mat_to_sp_tensor_np2(self, X):
    coo = X.tocoo().astype(np.float32)
    indices = np.stack([coo.row, coo.col], axis=1)  # (nnz, 2)
    return tf.SparseTensor(indices, coo.data, coo.shape)

LightGCN._convert_sp_mat_to_sp_tensor = _convert_sp_mat_to_sp_tensor_np2

# --- Load pretrained embeddings exported from PyTorch phase ---
user_emb_np = np.load("pretrained_user_emb.npy")  # (n_users_pre, emb_dim)
item_emb_np = np.load("pretrained_item_emb.npy")  # (n_items_pre, emb_dim)
emb_dim = user_emb_np.shape[1]
print("Loaded pretrained embeddings:", user_emb_np.shape, item_emb_np.shape)


def create_sub_file_pretrained(name="team_lightgcn_pretrained.txt", n_layers=2):
    os.makedirs("subs", exist_ok=True)

    # Same as before: train_full = train_df + test_df
    train_full = pd.concat([train_df, test_df], ignore_index=True)

    data_full = ImplicitCF(
        train=train_full,
        test=None,
        adj_dir=None,
        seed=1337,
    )

    hparams_pre = prepare_hparams(
        yaml_file=None,
        MODEL_DIR="",
        model_type="lightgcn",
        eval_epoch=1000,
        embed_size=emb_dim,   # MUST match pretrained emb dim
        n_layers=n_layers,
        decay=1e-4,
        learning_rate=0.001,
        batch_size=2048,
        epochs=100,
        top_k=20,
        metrics=["ndcg", "recall"],
        save_model=False,
        save_epoch=100,
    )

    # Build LightGCN (TF model)
    model_pre = LightGCN(hparams_pre, data_full, seed=1337)

    print("LightGCN users/items:", model_pre.n_users, model_pre.n_items)

    # --- Build internal-space embedding matrices ---
    user_emb_internal = np.zeros((model_pre.n_users, emb_dim), dtype=np.float32)
    item_emb_internal = np.zeros((model_pre.n_items, emb_dim), dtype=np.float32)

    # Fill users: internal_id -> original user ID -> pretrained row
    for internal_uid in range(model_pre.n_users):
        orig_uid = data_full.id2user[internal_uid]  # internal -> original
        if orig_uid < user_emb_np.shape[0]:
            user_emb_internal[internal_uid] = user_emb_np[orig_uid]
        else:
            user_emb_internal[internal_uid] = np.random.normal(
                scale=0.01, size=emb_dim
            ).astype(np.float32)

    # Fill items: internal_id -> original item ID -> pretrained row
    for internal_iid in range(model_pre.n_items):
        orig_iid = data_full.id2item[internal_iid]  # internal -> original
        if orig_iid < item_emb_np.shape[0]:
            item_emb_internal[internal_iid] = item_emb_np[orig_iid]
        else:
            item_emb_internal[internal_iid] = np.random.normal(
                scale=0.01, size=emb_dim
            ).astype(np.float32)

    print("Built internal user/item embedding matrices:",
          user_emb_internal.shape, item_emb_internal.shape)

    # --- Overwrite LightGCN's Xavier embeddings with our internal ones ---
    model_pre.sess.run(
        model_pre.weights["user_embedding"].assign(user_emb_internal)
    )
    model_pre.sess.run(
        model_pre.weights["item_embedding"].assign(item_emb_internal)
    )
    print("Copied pretrained embeddings into LightGCN TF variables (aligned by id2user/id2item).")

    # --- Train LightGCN from pretrained init ---
    print("Training LightGCN with PRETRAINED initialization...")
    model_pre.fit()

    # --- Build submission file ---
    user_ids = train_full[DEFAULT_USER_COL].unique()
    user_ids.sort()

    def join_items(x):
        return " ".join(str(i) for i in x.tolist())

    out_path = f"subs/{name}"
    chunk_size = 5000

    with open(out_path, "w") as f:
        for start in range(0, len(user_ids), chunk_size):
            end = start + chunk_size
            batch_ids = user_ids[start:end]
            batch_users = pd.DataFrame({DEFAULT_USER_COL: batch_ids})

            batch_recs = model_pre.recommend_k_items(
                test=batch_users,
                top_k=20,
                sort_top_k=True,
                remove_seen=True,
                use_id=False,  # same as your baseline
            )

            batch_submission = (
                batch_recs.sort_values(
                    [DEFAULT_USER_COL, DEFAULT_PREDICTION_COL],
                    ascending=[True, False],
                )
                .groupby(DEFAULT_USER_COL)[DEFAULT_ITEM_COL]
                .apply(join_items)
                .reset_index()
            )

            for _, row in batch_submission.iterrows():
                f.write(f"{int(row[DEFAULT_USER_COL])} {row[DEFAULT_ITEM_COL]}\n")

    print(f"Pretrained-LightGCN submission written to: {out_path}")


# Run once
create_sub_file_pretrained("team_lightgcn_pretrained.txt")

